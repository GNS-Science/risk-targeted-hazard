{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from risk_targeted_hazard import *  # not yet a pypi package, must be installed locally\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### prep query from NSHM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hazard_id = 'NSHM_v1.0.4'\n",
    "\n",
    "hf_name = 'test.hdf5'\n",
    "all_sites = False   # returns list of 214 named and 3600 gridded sites\n",
    "named_sites = True  # only returns named sites--returns all 214 if site_list=None or the subset included in site_list, see below\n",
    "short_list = False  # a small subset of vs30, imt, agg. see below.\n",
    "\n",
    "# sites of interest\n",
    "if all_sites:\n",
    "    sites = pd.concat([create_sites_df(),create_sites_df(named_sites=False)])\n",
    "elif named_sites:\n",
    "    site_list = None # ['Auckland','Wellington']\n",
    "    site_list = ['Auckland','Blenheim','Christchurch','Dunedin','Gisborne','Greymouth','Masterton','Napier','Nelson',\n",
    "                'Queenstown','Tauranga','Wellington']\n",
    "    sites = create_sites_df(named_sites=named_sites,site_list=site_list)\n",
    "else:\n",
    "    cropped_grid = False\n",
    "    grid_limits = [-38,np.inf,-np.inf,176] # [min_lat,max_lat,min_lon,max_lon]\n",
    "    sites = create_sites_df(named_sites=named_sites,cropped_grid=cropped_grid,grid_limits=grid_limits)\n",
    "\n",
    "# lists for vs30, imt, and aggregation statistics\n",
    "if short_list:\n",
    "    vs30_list = [275,375]\n",
    "    imt_list = ['SA(0.5)', 'SA(1.5)']\n",
    "    agg_list = [\"mean\",\"0.1\",\"0.5\",\"0.9\"]\n",
    "    \n",
    "else:\n",
    "    vs30_list = [150, 175, 225, 250, 275, 375, 400, 525, 750]\n",
    "    imt_list = ['PGA', 'SA(0.1)', 'SA(0.15)', 'SA(0.2)', 'SA(0.25)', 'SA(0.3)', 'SA(0.35)', 'SA(0.4)', 'SA(0.5)', 'SA(0.6)', 'SA(0.7)', 'SA(0.8)', 'SA(0.9)', 'SA(1.0)', 'SA(1.25)', 'SA(1.5)', 'SA(1.75)', 'SA(2.0)', 'SA(2.5)', 'SA(3.0)', 'SA(3.5)', 'SA(4.0)', 'SA(4.5)', 'SA(5.0)', 'SA(6.0)','SA(7.5)', 'SA(10.0)']\n",
    "    agg_list = [\"mean\",\"0.1\",\"0.5\",\"0.9\"]\n",
    "    \n",
    "\n",
    "# call a location to get the imtls that are returned\n",
    "res = next(get_hazard_curves(sites['latlon'][:1], vs30_list[:1], [hazard_id], imt_list[:1], agg_list[:1]))\n",
    "imtl_list = [float(val.lvl) for val in res.values]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### create the hdf5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# create a temporary dictionary\n",
    "data = {}\n",
    "\n",
    "# prep metadata\n",
    "imtls = {}\n",
    "for imt in imt_list:\n",
    "    imtls[imt] = imtl_list\n",
    "\n",
    "data['metadata'] = {}\n",
    "data['metadata']['quantiles'] = [float(q) for q in agg_list[1:]]\n",
    "data['metadata']['acc_imtls'] = imtls\n",
    "data['metadata']['disp_imtls'] = convert_imtls_to_disp(imtls)\n",
    "data['metadata']['sites'] = sites\n",
    "data['metadata']['vs30s'] = vs30_list\n",
    "\n",
    "# query the nshm\n",
    "data['hcurves'] = {}\n",
    "print('Querying hazard curves...')\n",
    "data['hcurves']['hcurves_stats'] = query_nshm_hcurves(hazard_id, sites, vs30_list, imt_list, imtl_list, agg_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_uniform_hazard_spectra(data,hazard_rps=np.array([25,50,100,250,500,1000,2500]))\n",
    "save_hdf(hf_name, data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### add risk spectra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "R_assumptions = {}\n",
    "\n",
    "####\n",
    "baseline_rp = 500\n",
    "baseline_risk = 1.5e-5\n",
    "cmr = 4\n",
    "####\n",
    "\n",
    "beta = 0.45\n",
    "design_point = stats.lognorm(beta,scale=cmr).cdf(1)\n",
    "p_fatality_given_collapse = 0.1\n",
    "\n",
    "R_assumptions['baseline_risk'] = baseline_risk\n",
    "R_assumptions['R_rps'] = [500,1000,2500]\n",
    "for R_rp in R_assumptions['R_rps']:\n",
    "    risk_factor = baseline_rp/R_rp\n",
    "    fatality_risk_target = round(baseline_risk*risk_factor,12)\n",
    "    R_assumptions[f'APoE: 1/{R_rp}'] = {'risk_factor': risk_factor,'fatality_risk_target': fatality_risk_target}\n",
    "\n",
    "R_assumptions\n",
    "\n",
    "\n",
    "risk_assumptions = {}\n",
    "for R_rp in R_assumptions['R_rps']:\n",
    "    key = f'APoE: 1/{R_rp}'\n",
    "    risk_assumptions[key] = { 'risk_factor':R_assumptions[f'APoE: 1/{R_rp}']['risk_factor'],\n",
    "                              'fatality_risk_target':R_assumptions[f'APoE: 1/{R_rp}']['fatality_risk_target'],\n",
    "                              'R_rp':R_rp,\n",
    "                              'ls_risk_target': round(R_assumptions[f'APoE: 1/{R_rp}']['fatality_risk_target'] / p_fatality_given_collapse,12),\n",
    "                              'cmr':cmr,\n",
    "                              'beta':beta,\n",
    "                              'design_point':design_point,\n",
    "                              'p_fatality_given_collapse':p_fatality_given_collapse\n",
    "                               }\n",
    "    \n",
    "risk_assumptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get risk values\n",
    "print('Calculating risk informed values.')\n",
    "data['risk_design'] = {}\n",
    "intensity_type = 'acc'\n",
    "data['risk_design'][intensity_type] = {}\n",
    "data['risk_design'][intensity_type]['risk_assumptions'] = risk_assumptions\n",
    "data['risk_design'][intensity_type]['im_risk'] = calculate_risk_design_intensities(data,risk_assumptions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "update_hdf_for_risk(hf_name,data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "toshi_cache",
   "language": "python",
   "name": "toshi_cache"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
