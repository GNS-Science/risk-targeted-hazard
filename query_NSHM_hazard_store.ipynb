{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from risk_targeted_hazard import *  # note yet a pypi package, must be installed locally\n",
    "\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "hazard_id = 'SLT_v8_gmm_v2_FINAL'\n",
    "\n",
    "res = next(get_hazard_curves(['-36.870~174.770'], [400], [hazard_id], ['PGA'], ['mean']))\n",
    "num_levels = len(res.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### query data for a short list of sites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "if True:\n",
    "    custom_location_dict = LOCATIONS_BY_ID.copy()\n",
    "    del custom_location_dict['WRE']\n",
    "    locations = [f\"{loc['latitude']:0.3f}~{loc['longitude']:0.3f}\" for loc in custom_location_dict.values()]\n",
    "\n",
    "    vs30_list = [150,175,200,225,250,275,300,350,375,400,450,500,600,750,900,1000,1500]\n",
    "    imts = ['PGA', 'SA(0.1)', 'SA(0.2)', 'SA(0.3)', 'SA(0.4)', 'SA(0.5)', 'SA(0.7)','SA(1.0)', 'SA(1.5)', 'SA(2.0)', 'SA(3.0)', 'SA(4.0)', 'SA(5.0)', 'SA(6.0)','SA(7.5)', 'SA(10.0)']\n",
    "    # aggs = [\"mean\", \"cov\", \"std\", \"0.005\", \"0.01\", \"0.025\", \"0.05\", \"0.1\", \"0.2\", \"0.5\", \"0.8\", \"0.9\", \"0.95\", \"0.975\", \"0.99\", \"0.995\"]\n",
    "    aggs = [\"mean\",\"0.1\",\"0.5\",\"0.9\"]\n",
    "    \n",
    "else:\n",
    "    # shorter list for testing\n",
    "    site_codes = ['AKL','WLG']\n",
    "    custom_location_dict = LOCATIONS_BY_ID.copy()\n",
    "    new_custom_location_dict = {}\n",
    "    for site_code in site_codes:\n",
    "        new_custom_location_dict[site_code] = custom_location_dict[site_code]\n",
    "    custom_location_dict = new_custom_location_dict\n",
    "    locations = [f\"{loc['latitude']:0.3f}~{loc['longitude']:0.3f}\" for loc in custom_location_dict.values()]\n",
    "\n",
    "    vs30_list = [250,400]\n",
    "    imts = ['SA(0.5)', 'SA(1.5)']\n",
    "    # aggs = [\"mean\", \"cov\", \"std\", \"0.005\", \"0.01\", \"0.025\", \"0.05\", \"0.1\", \"0.2\", \"0.5\", \"0.8\", \"0.9\", \"0.95\", \"0.975\", \"0.99\", \"0.995\"]\n",
    "    aggs = [\"mean\",\"0.1\",\"0.5\",\"0.9\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "columns = ['lat', 'lon', 'vs30','imt', 'agg', 'level', 'hazard']\n",
    "index = range(len(locations) * len(vs30_list) * len(imts) * len(aggs) * num_levels)\n",
    "pts_summary_data = pd.DataFrame(columns=columns, index=index)\n",
    "\n",
    "ind = 0\n",
    "for i,res in enumerate(get_hazard_curves(locations, vs30_list, [hazard_id], imts, aggs)):\n",
    "    lat = f'{res.lat:0.3f}'\n",
    "    lon = f'{res.lon:0.3f}'\n",
    "    for value in res.values:\n",
    "        pts_summary_data.loc[ind,'lat'] = lat\n",
    "        pts_summary_data.loc[ind,'lon'] = lon\n",
    "        pts_summary_data.loc[ind,'vs30'] = res.vs30\n",
    "        pts_summary_data.loc[ind,'imt'] = res.imt\n",
    "        pts_summary_data.loc[ind,'agg'] = res.agg\n",
    "        pts_summary_data.loc[ind,'level'] = value.lvl\n",
    "        pts_summary_data.loc[ind,'hazard'] = value.val\n",
    "        ind += 1\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "site_list = sorted([loc['name'] for loc in custom_location_dict.values()])\n",
    "sites = pd.DataFrame(index=site_list,dtype='str')\n",
    "for loc in custom_location_dict.values():\n",
    "    idx = (pts_summary_data['lat'].astype('float')==loc['latitude'])&(pts_summary_data['lon'].astype('float')==loc['longitude'])\n",
    "    pts_summary_data.loc[idx,'name'] = loc['name']\n",
    "    sites.loc[loc['name'],['lat','lon']] = [loc['latitude'],loc['longitude']]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### query data for a gridded map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vs30_list = [250,400]\n",
    "imts = ['PGA', 'SA(0.5)', 'SA(1.0)', 'SA(1.5)', 'SA(2.0)']\n",
    "# aggs = [\"mean\", \"cov\", \"std\", \"0.005\", \"0.01\", \"0.025\", \"0.05\", \"0.1\", \"0.2\", \"0.5\", \"0.8\", \"0.9\", \"0.95\", \"0.975\", \"0.99\", \"0.995\"]\n",
    "aggs = [\"mean\", \"0.1\", \"0.5\", \"0.9\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "site_list = 'NZ_0_1_NB_1_1'\n",
    "resample = 0.1\n",
    "locations = []\n",
    "grid = RegionGrid[site_list]\n",
    "grid_locs = grid.load()\n",
    "\n",
    "# remove empty location\n",
    "l = grid_locs.index( (-34.7,172.7) )\n",
    "grid_locs = grid_locs[0:l] + grid_locs[l+1:]\n",
    "\n",
    "for gloc in grid_locs:\n",
    "    loc = CodedLocation(*gloc, resolution=0.001)\n",
    "    loc = loc.resample(float(resample)) if resample else loc\n",
    "    locations.append(loc.resample(0.001).code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "columns = ['lat', 'lon', 'vs30','imt', 'agg', 'level', 'hazard']\n",
    "index = range(len(locations) * len(vs30_list) * len(imts) * len(aggs) * num_levels)\n",
    "grid_summary_data = pd.DataFrame(columns=columns, index=index)\n",
    "\n",
    "ind = 0\n",
    "for i,res in enumerate(get_hazard_curves(locations, vs30_list, [hazard_id], imts, aggs)):\n",
    "    lat = f'{res.lat:0.3f}'\n",
    "    lon = f'{res.lon:0.3f}'\n",
    "    for value in res.values:\n",
    "        grid_summary_data.loc[ind,'lat'] = lat\n",
    "        grid_summary_data.loc[ind,'lon'] = lon\n",
    "        grid_summary_data.loc[ind,'vs30'] = res.vs30\n",
    "        grid_summary_data.loc[ind,'imt'] = res.imt\n",
    "        grid_summary_data.loc[ind,'agg'] = res.agg\n",
    "        grid_summary_data.loc[ind,'level'] = value.lvl\n",
    "        grid_summary_data.loc[ind,'hazard'] = value.val\n",
    "        ind += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### compile data for either the short list ('points') or for the gridded sites ('grid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_type = 'points'\n",
    "\n",
    "if data_type=='grid':\n",
    "    site_label = '_grid-0pt1'\n",
    "    summary_data = grid_summary_data\n",
    "    vs30_list = list(np.unique(summary_data['vs30'].dropna()))\n",
    "    \n",
    "    site_list = 'NZ_0_1_NB_1_1'\n",
    "    resample = 0.1\n",
    "    locations = []\n",
    "    grid = RegionGrid[site_list]\n",
    "    grid_locs = grid.load()\n",
    "\n",
    "    # remove empty location\n",
    "    l = grid_locs.index( (-34.7,172.7) )\n",
    "    grid_locs = grid_locs[0:l] + grid_locs[l+1:]\n",
    "\n",
    "    for gloc in grid_locs:\n",
    "        loc = CodedLocation(*gloc, resolution=0.001)\n",
    "        loc = loc.resample(float(resample)) if resample else loc\n",
    "        locations.append(loc.resample(0.001).code)\n",
    "        \n",
    "    sites = pd.DataFrame(dtype='str')\n",
    "    for i,latlon in enumerate(locations):\n",
    "        lat,lon = latlon.split('~')\n",
    "        sites.loc[i,['lat','lon']] = [float(lat),float(lon)]\n",
    "    sites['lat/lon'] = [(lat,lon) for lat,lon in zip(sites['lat'],sites['lon'])]\n",
    "\n",
    "    \n",
    "else:\n",
    "    site_label=''\n",
    "    summary_data = pts_summary_data\n",
    "    vs30_list = list(np.unique(summary_data['vs30'].dropna()))\n",
    "        \n",
    "    site_list = sorted([loc['name'] for loc in custom_location_dict.values()])\n",
    "    sites = pd.DataFrame(index=site_list,dtype='str')\n",
    "    for loc in custom_location_dict.values():\n",
    "        idx = (pts_summary_data['lat'].astype('float')==loc['latitude'])&(pts_summary_data['lon'].astype('float')==loc['longitude'])\n",
    "        pts_summary_data.loc[idx,'name'] = loc['name']\n",
    "        sites.loc[loc['name'],['lat','lon']] = [loc['latitude'],loc['longitude']]\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "imtl = list(np.unique(summary_data['level'].dropna()))\n",
    "imtls = {}\n",
    "for imt in imts:\n",
    "    imtls[imt] = imtl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {}\n",
    "data['metadata'] = {}\n",
    "data['metadata']['quantiles'] = []\n",
    "data['metadata']['acc_imtls'] = imtls\n",
    "data['metadata']['sites'] = sites\n",
    "data['metadata']['vs30s'] = vs30_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_dict = {}\n",
    "\n",
    "idx_dict['vs30'] = {}\n",
    "for i_vs30,vs30 in enumerate(vs30_list):\n",
    "    idx_dict['vs30'][vs30] = summary_data['vs30']==vs30\n",
    "    \n",
    "if data_type=='grid':\n",
    "    idx_dict['lat'] = {}\n",
    "    for i_lat,lat in enumerate(np.unique(sites['lat'])):\n",
    "        idx_dict['lat'][lat] = summary_data['lat']==f'{float(lat):.3f}'\n",
    "\n",
    "    idx_dict['lon'] = {}\n",
    "    for i_lon,lon in enumerate(np.unique(sites['lon'])):\n",
    "        idx_dict['lon'][lon] = summary_data['lon']==f'{float(lon):.3f}'\n",
    "else:\n",
    "    idx_dict['site'] = {}\n",
    "    for i_site,site in enumerate(sites.index):\n",
    "        idx_dict['site'][site] = summary_data['name']==site\n",
    "    \n",
    "idx_dict['imt'] = {}\n",
    "for i_imt,imt in enumerate(imts):\n",
    "    idx_dict['imt'][imt] = summary_data['imt']==imt\n",
    "\n",
    "idx_dict['stat'] = {}\n",
    "for i_stat,stat in enumerate(aggs):\n",
    "    idx_dict['stat'][stat] = summary_data['agg']==stat "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vs30: 250\n",
      "\tSite: Auckland\n",
      "\t\tIMT: SA(0.5)\t\t(Vs30: 250 at Site: Auckland  #1 of 2)\n",
      "\t\t\tStat: mean\n",
      "\t\t\tStat: 0.1\n",
      "\t\t\tStat: 0.5\n",
      "\t\t\tStat: 0.9\n",
      "\t\tIMT: SA(1.5)\t\t(Vs30: 250 at Site: Auckland  #1 of 2)\n",
      "\t\t\tStat: mean\n",
      "\t\t\tStat: 0.1\n",
      "\t\t\tStat: 0.5\n",
      "\t\t\tStat: 0.9\n",
      "\tSite: Wellington\n",
      "\t\tIMT: SA(0.5)\t\t(Vs30: 250 at Site: Wellington  #2 of 2)\n",
      "\t\t\tStat: mean\n",
      "\t\t\tStat: 0.1\n",
      "\t\t\tStat: 0.5\n",
      "\t\t\tStat: 0.9\n",
      "\t\tIMT: SA(1.5)\t\t(Vs30: 250 at Site: Wellington  #2 of 2)\n",
      "\t\t\tStat: mean\n",
      "\t\t\tStat: 0.1\n",
      "\t\t\tStat: 0.5\n",
      "\t\t\tStat: 0.9\n",
      "Vs30: 400\n",
      "\tSite: Auckland\n",
      "\t\tIMT: SA(0.5)\t\t(Vs30: 400 at Site: Auckland  #1 of 2)\n",
      "\t\t\tStat: mean\n",
      "\t\t\tStat: 0.1\n",
      "\t\t\tStat: 0.5\n",
      "\t\t\tStat: 0.9\n",
      "\t\tIMT: SA(1.5)\t\t(Vs30: 400 at Site: Auckland  #1 of 2)\n",
      "\t\t\tStat: mean\n",
      "\t\t\tStat: 0.1\n",
      "\t\t\tStat: 0.5\n",
      "\t\t\tStat: 0.9\n",
      "\tSite: Wellington\n",
      "\t\tIMT: SA(0.5)\t\t(Vs30: 400 at Site: Wellington  #2 of 2)\n",
      "\t\t\tStat: mean\n",
      "\t\t\tStat: 0.1\n",
      "\t\t\tStat: 0.5\n",
      "\t\t\tStat: 0.9\n",
      "\t\tIMT: SA(1.5)\t\t(Vs30: 400 at Site: Wellington  #2 of 2)\n",
      "\t\t\tStat: mean\n",
      "\t\t\tStat: 0.1\n",
      "\t\t\tStat: 0.5\n",
      "\t\t\tStat: 0.9\n"
     ]
    }
   ],
   "source": [
    "hcurves = np.zeros([len(vs30_list),len(locations),len(imts),len(imtl),len(aggs)])\n",
    "\n",
    "for i_vs30,vs30 in enumerate(vs30_list):\n",
    "    print(f'Vs30: {vs30}')\n",
    "    vs30_idx = idx_dict['vs30'][vs30]\n",
    "    \n",
    "    if data_type == 'grid':\n",
    "        for i_latlon,latlon in enumerate(locations):\n",
    "            print(f'\\tLat/Lon: {latlon}')\n",
    "            lat,lon = latlon.split('~')\n",
    "            latlon_idx = (idx_dict['lat'][lat]) & (idx_dict['lon'][lon])\n",
    "\n",
    "            for i_imt,imt in enumerate(imts):\n",
    "                print(f'\\t\\tIMT: {imt}\\t\\t(Vs30: {vs30} at Lat/Lon: {latlon}  #{i_latlon+1} of {len(locations)})')\n",
    "                imt_idx = idx_dict['imt'][imt]\n",
    "\n",
    "                for i_stat,stat in enumerate(aggs):\n",
    "                    print(f'\\t\\t\\tStat: {stat}')\n",
    "                    stat_idx = idx_dict['stat'][stat]\n",
    "\n",
    "                    idx = vs30_idx & latlon_idx & imt_idx & stat_idx\n",
    "                    hcurve = np.squeeze(summary_data[idx]['hazard'].to_numpy())\n",
    "\n",
    "                    hcurves[i_vs30,i_latlon,i_imt,:,i_stat] = hcurve\n",
    "                    \n",
    "    else:\n",
    "        for i_site,site in enumerate(sites.index):\n",
    "            print(f'\\tSite: {site}')\n",
    "            site_idx = idx_dict['site'][site]\n",
    "\n",
    "            for i_imt,imt in enumerate(imts):\n",
    "                print(f'\\t\\tIMT: {imt}\\t\\t(Vs30: {vs30} at Site: {site}  #{i_site+1} of {len(sites)})')\n",
    "                imt_idx = idx_dict['imt'][imt]\n",
    "\n",
    "                for i_stat,stat in enumerate(aggs):\n",
    "                    print(f'\\t\\t\\tStat: {stat}')\n",
    "                    stat_idx = idx_dict['stat'][stat]\n",
    "\n",
    "                    idx = vs30_idx & site_idx & imt_idx & stat_idx\n",
    "                    hcurve = np.squeeze(summary_data[idx]['hazard'].to_numpy())\n",
    "\n",
    "                    hcurves[i_vs30,i_site,i_imt,:,i_stat] = hcurve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['hcurves'] = {}\n",
    "data['hcurves']['hcurves_stats'] = hcurves"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### process data for the uniform hazard design intensities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating PoE values.\n"
     ]
    }
   ],
   "source": [
    "data['metadata']['disp_imtls'] = convert_imtls_to_disp(imtls)\n",
    "data['metadata']['quantiles'] = [float(q) for q in aggs[1:]]\n",
    "\n",
    "# get poe values\n",
    "print('Calculating PoE values.')\n",
    "hazard_rps = np.array([25,50,100,250,500,1000,2500])\n",
    "data['hazard_design'] = {}\n",
    "data['hazard_design']['hazard_rps'] = hazard_rps.tolist()\n",
    "\n",
    "for intensity_type in ['acc','disp']:\n",
    "    data['hazard_design'][intensity_type] = {}\n",
    "    data['hazard_design'][intensity_type]['stats_im_hazard'] = calculate_hazard_design_intensities(data,hazard_rps,intensity_type)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if data_type=='grid':\n",
    "    hf_name = f'v3_cmr-{cmr}_vs30-{vs30_list}_imt-{imts}{site_label}.hdf5'\n",
    "else:\n",
    "    hf_name = f'v3_cmr-{cmr}_vs30-all_R-factors.hdf5'\n",
    "save_hdf(hf_name,data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### process data for the risk informed design intensities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'baseline_risk': 1.5e-05,\n",
       " 'R_rps': [500, 1000, 2500],\n",
       " 'APoE: 1/500': {'risk_factor': 1.0, 'risk_target': 1.5e-05},\n",
       " 'APoE: 1/1000': {'risk_factor': 0.5, 'risk_target': 7.5e-06},\n",
       " 'APoE: 1/2500': {'risk_factor': 0.2, 'risk_target': 3e-06}}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "{'APoE: 1/500': {'risk_factor': 1.0,\n",
       "  'risk_target': 1.5e-05,\n",
       "  'R_rp': 500,\n",
       "  'collapse_risk_target': 0.00015,\n",
       "  'cmr': 4,\n",
       "  'beta': 0.45,\n",
       "  'design_point': 0.001032732090754596,\n",
       "  'p_fatality_given_collapse': 0.1},\n",
       " 'APoE: 1/1000': {'risk_factor': 0.5,\n",
       "  'risk_target': 7.5e-06,\n",
       "  'R_rp': 1000,\n",
       "  'collapse_risk_target': 7.5e-05,\n",
       "  'cmr': 4,\n",
       "  'beta': 0.45,\n",
       "  'design_point': 0.001032732090754596,\n",
       "  'p_fatality_given_collapse': 0.1},\n",
       " 'APoE: 1/2500': {'risk_factor': 0.2,\n",
       "  'risk_target': 3e-06,\n",
       "  'R_rp': 2500,\n",
       "  'collapse_risk_target': 3e-05,\n",
       "  'cmr': 4,\n",
       "  'beta': 0.45,\n",
       "  'design_point': 0.001032732090754596,\n",
       "  'p_fatality_given_collapse': 0.1}}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "R_assumptions = {}\n",
    "\n",
    "####\n",
    "baseline_rp = 500\n",
    "baseline_risk = 1.5e-5\n",
    "cmr = 4\n",
    "####\n",
    "\n",
    "beta = 0.45\n",
    "design_point = stats.lognorm(beta,scale=cmr).cdf(1)\n",
    "p_fatality_given_collapse = 0.1\n",
    "\n",
    "R_assumptions['baseline_risk'] = baseline_risk\n",
    "R_assumptions['R_rps'] = [500,1000,2500]\n",
    "for R_rp in R_assumptions['R_rps']:\n",
    "    risk_factor = baseline_rp/R_rp\n",
    "    risk_target = round(baseline_risk*risk_factor,12)\n",
    "    R_assumptions[f'APoE: 1/{R_rp}'] = {'risk_factor': risk_factor,'risk_target': risk_target}\n",
    "\n",
    "R_assumptions\n",
    "\n",
    "\n",
    "risk_assumptions = {}\n",
    "for R_rp in R_assumptions['R_rps']:\n",
    "    key = f'APoE: 1/{R_rp}'\n",
    "    risk_assumptions[key] = { 'risk_factor':R_assumptions[f'APoE: 1/{R_rp}']['risk_factor'],\n",
    "                              'risk_target':R_assumptions[f'APoE: 1/{R_rp}']['risk_target'],\n",
    "                              'R_rp':R_rp,\n",
    "                              'collapse_risk_target': round(R_assumptions[f'APoE: 1/{R_rp}']['risk_target'] / p_fatality_given_collapse,12),\n",
    "                              'cmr':cmr,\n",
    "                              'beta':beta,\n",
    "                              'design_point':design_point,\n",
    "                              'p_fatality_given_collapse':p_fatality_given_collapse\n",
    "                               }\n",
    "    \n",
    "risk_assumptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating risk informed values.\n",
      "Processing Vs30: 250.\n",
      "\tProcessing SA(0.5).\n",
      "\tProcessing SA(1.5).\n",
      "\n",
      "Processing Vs30: 400.\n",
      "\tProcessing SA(0.5).\n",
      "\tProcessing SA(1.5).\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# get risk values\n",
    "print('Calculating risk informed values.')\n",
    "data['risk_design'] = {}\n",
    "intensity_type = 'acc'\n",
    "data['risk_design'][intensity_type] = {}\n",
    "data['risk_design'][intensity_type]['risk_assumptions'] = risk_assumptions\n",
    "data['risk_design'][intensity_type]['im_risk'] = calculate_risk_design_intensities(data,risk_assumptions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "update_hdf_for_risk(hf_name,data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310",
   "language": "python",
   "name": "py310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
