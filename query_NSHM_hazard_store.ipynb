{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# from toshi_hazard_store.query_v3 import get_hazard_curves\n",
    "# from nzshm_common.location.location import LOCATIONS_BY_ID\n",
    "# from nzshm_common.location.code_location import CodedLocation\n",
    "# from nzshm_common.location import location\n",
    "# from nzshm_common.grids import RegionGrid\n",
    "\n",
    "from risk_targeted_hazard import *\n",
    "# import re\n",
    "# import ast\n",
    "\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "hazard_id = 'SLT_v8_gmm_v2_FINAL'\n",
    "\n",
    "res = next(get_hazard_curves(['-36.870~174.770'], [400], [hazard_id], ['PGA'], ['mean']))\n",
    "num_levels = len(res.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### short list of sites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "if True:\n",
    "    custom_location_dict = LOCATIONS_BY_ID.copy()\n",
    "    del custom_location_dict['WRE']\n",
    "    locations = [f\"{loc['latitude']:0.3f}~{loc['longitude']:0.3f}\" for loc in custom_location_dict.values()]\n",
    "\n",
    "    vs30_list = [150,175,200,225,250,275,300,350,375,400,450,500,600,750,900,1000,1500]\n",
    "    imts = ['PGA', 'SA(0.1)', 'SA(0.2)', 'SA(0.3)', 'SA(0.4)', 'SA(0.5)', 'SA(0.7)','SA(1.0)', 'SA(1.5)', 'SA(2.0)', 'SA(3.0)', 'SA(4.0)', 'SA(5.0)', 'SA(6.0)','SA(7.5)', 'SA(10.0)']\n",
    "    # aggs = [\"mean\", \"cov\", \"std\", \"0.005\", \"0.01\", \"0.025\", \"0.05\", \"0.1\", \"0.2\", \"0.5\", \"0.8\", \"0.9\", \"0.95\", \"0.975\", \"0.99\", \"0.995\"]\n",
    "    aggs = [\"mean\",\"0.1\",\"0.5\",\"0.9\"]\n",
    "    \n",
    "else:\n",
    "    # shorter list for testing\n",
    "    site_codes = ['AKL','WLG']\n",
    "    custom_location_dict = LOCATIONS_BY_ID.copy()\n",
    "    new_custom_location_dict = {}\n",
    "    for site_code in site_codes:\n",
    "        new_custom_location_dict[site_code] = custom_location_dict[site_code]\n",
    "    custom_location_dict = new_custom_location_dict\n",
    "    locations = [f\"{loc['latitude']:0.3f}~{loc['longitude']:0.3f}\" for loc in custom_location_dict.values()]\n",
    "\n",
    "    vs30_list = [250,400]\n",
    "    imts = ['SA(0.5)', 'SA(1.5)']\n",
    "    # aggs = [\"mean\", \"cov\", \"std\", \"0.005\", \"0.01\", \"0.025\", \"0.05\", \"0.1\", \"0.2\", \"0.5\", \"0.8\", \"0.9\", \"0.95\", \"0.975\", \"0.99\", \"0.995\"]\n",
    "    aggs = [\"mean\",\"0.1\",\"0.5\",\"0.9\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "columns = ['lat', 'lon', 'vs30','imt', 'agg', 'level', 'hazard']\n",
    "index = range(len(locations) * len(vs30_list) * len(imts) * len(aggs) * num_levels)\n",
    "pts_summary_data = pd.DataFrame(columns=columns, index=index)\n",
    "\n",
    "ind = 0\n",
    "for i,res in enumerate(get_hazard_curves(locations, vs30_list, [hazard_id], imts, aggs)):\n",
    "    lat = f'{res.lat:0.3f}'\n",
    "    lon = f'{res.lon:0.3f}'\n",
    "    for value in res.values:\n",
    "        pts_summary_data.loc[ind,'lat'] = lat\n",
    "        pts_summary_data.loc[ind,'lon'] = lon\n",
    "        pts_summary_data.loc[ind,'vs30'] = res.vs30\n",
    "        pts_summary_data.loc[ind,'imt'] = res.imt\n",
    "        pts_summary_data.loc[ind,'agg'] = res.agg\n",
    "        pts_summary_data.loc[ind,'level'] = value.lvl\n",
    "        pts_summary_data.loc[ind,'hazard'] = value.val\n",
    "        ind += 1\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "site_list = sorted([loc['name'] for loc in custom_location_dict.values()])\n",
    "sites = pd.DataFrame(index=site_list,dtype='str')\n",
    "for loc in custom_location_dict.values():\n",
    "    idx = (pts_summary_data['lat'].astype('float')==loc['latitude'])&(pts_summary_data['lon'].astype('float')==loc['longitude'])\n",
    "    pts_summary_data.loc[idx,'name'] = loc['name']\n",
    "    sites.loc[loc['name'],['lat','lon']] = [loc['latitude'],loc['longitude']]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### gridded map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vs30_list = [250,400]\n",
    "imts = ['PGA', 'SA(0.5)', 'SA(1.0)', 'SA(1.5)', 'SA(2.0)']\n",
    "# aggs = [\"mean\", \"cov\", \"std\", \"0.005\", \"0.01\", \"0.025\", \"0.05\", \"0.1\", \"0.2\", \"0.5\", \"0.8\", \"0.9\", \"0.95\", \"0.975\", \"0.99\", \"0.995\"]\n",
    "aggs = [\"mean\", \"0.1\", \"0.5\", \"0.9\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "site_list = 'NZ_0_1_NB_1_1'\n",
    "resample = 0.1\n",
    "locations = []\n",
    "grid = RegionGrid[site_list]\n",
    "grid_locs = grid.load()\n",
    "\n",
    "# remove empty location\n",
    "l = grid_locs.index( (-34.7,172.7) )\n",
    "grid_locs = grid_locs[0:l] + grid_locs[l+1:]\n",
    "\n",
    "for gloc in grid_locs:\n",
    "    loc = CodedLocation(*gloc, resolution=0.001)\n",
    "    loc = loc.resample(float(resample)) if resample else loc\n",
    "    locations.append(loc.resample(0.001).code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "columns = ['lat', 'lon', 'vs30','imt', 'agg', 'level', 'hazard']\n",
    "index = range(len(locations) * len(vs30_list) * len(imts) * len(aggs) * num_levels)\n",
    "grid_summary_data = pd.DataFrame(columns=columns, index=index)\n",
    "\n",
    "ind = 0\n",
    "for i,res in enumerate(get_hazard_curves(locations, vs30_list, [hazard_id], imts, aggs)):\n",
    "    lat = f'{res.lat:0.3f}'\n",
    "    lon = f'{res.lon:0.3f}'\n",
    "    for value in res.values:\n",
    "        grid_summary_data.loc[ind,'lat'] = lat\n",
    "        grid_summary_data.loc[ind,'lon'] = lon\n",
    "        grid_summary_data.loc[ind,'vs30'] = res.vs30\n",
    "        grid_summary_data.loc[ind,'imt'] = res.imt\n",
    "        grid_summary_data.loc[ind,'agg'] = res.agg\n",
    "        grid_summary_data.loc[ind,'level'] = value.lvl\n",
    "        grid_summary_data.loc[ind,'hazard'] = value.val\n",
    "        ind += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### compile data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_type = 'points'\n",
    "\n",
    "if data_type=='grid':\n",
    "    site_label = '_grid-0pt1'\n",
    "    summary_data = grid_summary_data\n",
    "    vs30_list = list(np.unique(summary_data['vs30'].dropna()))\n",
    "    \n",
    "    site_list = 'NZ_0_1_NB_1_1'\n",
    "    resample = 0.1\n",
    "    locations = []\n",
    "    grid = RegionGrid[site_list]\n",
    "    grid_locs = grid.load()\n",
    "\n",
    "    # remove empty location\n",
    "    l = grid_locs.index( (-34.7,172.7) )\n",
    "    grid_locs = grid_locs[0:l] + grid_locs[l+1:]\n",
    "\n",
    "    for gloc in grid_locs:\n",
    "        loc = CodedLocation(*gloc, resolution=0.001)\n",
    "        loc = loc.resample(float(resample)) if resample else loc\n",
    "        locations.append(loc.resample(0.001).code)\n",
    "        \n",
    "    sites = pd.DataFrame(dtype='str')\n",
    "    for i,latlon in enumerate(locations):\n",
    "        lat,lon = latlon.split('~')\n",
    "        sites.loc[i,['lat','lon']] = [float(lat),float(lon)]\n",
    "    sites['lat/lon'] = [(lat,lon) for lat,lon in zip(sites['lat'],sites['lon'])]\n",
    "\n",
    "    \n",
    "else:\n",
    "    site_label=''\n",
    "    summary_data = pts_summary_data\n",
    "    vs30_list = list(np.unique(summary_data['vs30'].dropna()))\n",
    "        \n",
    "    site_list = sorted([loc['name'] for loc in custom_location_dict.values()])\n",
    "    sites = pd.DataFrame(index=site_list,dtype='str')\n",
    "    for loc in custom_location_dict.values():\n",
    "        idx = (pts_summary_data['lat'].astype('float')==loc['latitude'])&(pts_summary_data['lon'].astype('float')==loc['longitude'])\n",
    "        pts_summary_data.loc[idx,'name'] = loc['name']\n",
    "        sites.loc[loc['name'],['lat','lon']] = [loc['latitude'],loc['longitude']]\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "imtl = list(np.unique(summary_data['level'].dropna()))\n",
    "imtls = {}\n",
    "for imt in imts:\n",
    "    imtls[imt] = imtl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {}\n",
    "data['metadata'] = {}\n",
    "data['metadata']['quantiles'] = []\n",
    "data['metadata']['acc_imtls'] = imtls\n",
    "data['metadata']['sites'] = sites\n",
    "data['metadata']['vs30s'] = vs30_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_dict = {}\n",
    "\n",
    "idx_dict['vs30'] = {}\n",
    "for i_vs30,vs30 in enumerate(vs30_list):\n",
    "    idx_dict['vs30'][vs30] = summary_data['vs30']==vs30\n",
    "    \n",
    "if data_type=='grid':\n",
    "    idx_dict['lat'] = {}\n",
    "    for i_lat,lat in enumerate(np.unique(sites['lat'])):\n",
    "        idx_dict['lat'][lat] = summary_data['lat']==f'{float(lat):.3f}'\n",
    "\n",
    "    idx_dict['lon'] = {}\n",
    "    for i_lon,lon in enumerate(np.unique(sites['lon'])):\n",
    "        idx_dict['lon'][lon] = summary_data['lon']==f'{float(lon):.3f}'\n",
    "else:\n",
    "    idx_dict['site'] = {}\n",
    "    for i_site,site in enumerate(sites.index):\n",
    "        idx_dict['site'][site] = summary_data['name']==site\n",
    "    \n",
    "idx_dict['imt'] = {}\n",
    "for i_imt,imt in enumerate(imts):\n",
    "    idx_dict['imt'][imt] = summary_data['imt']==imt\n",
    "\n",
    "idx_dict['stat'] = {}\n",
    "for i_stat,stat in enumerate(aggs):\n",
    "    idx_dict['stat'][stat] = summary_data['agg']==stat "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vs30: 250\n",
      "\tSite: Auckland\n",
      "\t\tIMT: SA(0.5)\t\t(Vs30: 250 at Site: Auckland  #1 of 2)\n",
      "\t\t\tStat: mean\n",
      "\t\t\tStat: 0.1\n",
      "\t\t\tStat: 0.5\n",
      "\t\t\tStat: 0.9\n",
      "\t\tIMT: SA(1.5)\t\t(Vs30: 250 at Site: Auckland  #1 of 2)\n",
      "\t\t\tStat: mean\n",
      "\t\t\tStat: 0.1\n",
      "\t\t\tStat: 0.5\n",
      "\t\t\tStat: 0.9\n",
      "\tSite: Wellington\n",
      "\t\tIMT: SA(0.5)\t\t(Vs30: 250 at Site: Wellington  #2 of 2)\n",
      "\t\t\tStat: mean\n",
      "\t\t\tStat: 0.1\n",
      "\t\t\tStat: 0.5\n",
      "\t\t\tStat: 0.9\n",
      "\t\tIMT: SA(1.5)\t\t(Vs30: 250 at Site: Wellington  #2 of 2)\n",
      "\t\t\tStat: mean\n",
      "\t\t\tStat: 0.1\n",
      "\t\t\tStat: 0.5\n",
      "\t\t\tStat: 0.9\n",
      "Vs30: 400\n",
      "\tSite: Auckland\n",
      "\t\tIMT: SA(0.5)\t\t(Vs30: 400 at Site: Auckland  #1 of 2)\n",
      "\t\t\tStat: mean\n",
      "\t\t\tStat: 0.1\n",
      "\t\t\tStat: 0.5\n",
      "\t\t\tStat: 0.9\n",
      "\t\tIMT: SA(1.5)\t\t(Vs30: 400 at Site: Auckland  #1 of 2)\n",
      "\t\t\tStat: mean\n",
      "\t\t\tStat: 0.1\n",
      "\t\t\tStat: 0.5\n",
      "\t\t\tStat: 0.9\n",
      "\tSite: Wellington\n",
      "\t\tIMT: SA(0.5)\t\t(Vs30: 400 at Site: Wellington  #2 of 2)\n",
      "\t\t\tStat: mean\n",
      "\t\t\tStat: 0.1\n",
      "\t\t\tStat: 0.5\n",
      "\t\t\tStat: 0.9\n",
      "\t\tIMT: SA(1.5)\t\t(Vs30: 400 at Site: Wellington  #2 of 2)\n",
      "\t\t\tStat: mean\n",
      "\t\t\tStat: 0.1\n",
      "\t\t\tStat: 0.5\n",
      "\t\t\tStat: 0.9\n"
     ]
    }
   ],
   "source": [
    "hcurves = np.zeros([len(vs30_list),len(locations),len(imts),len(imtl),len(aggs)])\n",
    "\n",
    "for i_vs30,vs30 in enumerate(vs30_list):\n",
    "    print(f'Vs30: {vs30}')\n",
    "    vs30_idx = idx_dict['vs30'][vs30]\n",
    "    \n",
    "    if data_type == 'grid':\n",
    "        for i_latlon,latlon in enumerate(locations):\n",
    "            print(f'\\tLat/Lon: {latlon}')\n",
    "            lat,lon = latlon.split('~')\n",
    "            latlon_idx = (idx_dict['lat'][lat]) & (idx_dict['lon'][lon])\n",
    "\n",
    "            for i_imt,imt in enumerate(imts):\n",
    "                print(f'\\t\\tIMT: {imt}\\t\\t(Vs30: {vs30} at Lat/Lon: {latlon}  #{i_latlon+1} of {len(locations)})')\n",
    "                imt_idx = idx_dict['imt'][imt]\n",
    "\n",
    "                for i_stat,stat in enumerate(aggs):\n",
    "                    print(f'\\t\\t\\tStat: {stat}')\n",
    "                    stat_idx = idx_dict['stat'][stat]\n",
    "\n",
    "                    idx = vs30_idx & latlon_idx & imt_idx & stat_idx\n",
    "                    hcurve = np.squeeze(summary_data[idx]['hazard'].to_numpy())\n",
    "\n",
    "                    hcurves[i_vs30,i_latlon,i_imt,:,i_stat] = hcurve\n",
    "                    \n",
    "    else:\n",
    "        for i_site,site in enumerate(sites.index):\n",
    "            print(f'\\tSite: {site}')\n",
    "            site_idx = idx_dict['site'][site]\n",
    "\n",
    "            for i_imt,imt in enumerate(imts):\n",
    "                print(f'\\t\\tIMT: {imt}\\t\\t(Vs30: {vs30} at Site: {site}  #{i_site+1} of {len(sites)})')\n",
    "                imt_idx = idx_dict['imt'][imt]\n",
    "\n",
    "                for i_stat,stat in enumerate(aggs):\n",
    "                    print(f'\\t\\t\\tStat: {stat}')\n",
    "                    stat_idx = idx_dict['stat'][stat]\n",
    "\n",
    "                    idx = vs30_idx & site_idx & imt_idx & stat_idx\n",
    "                    hcurve = np.squeeze(summary_data[idx]['hazard'].to_numpy())\n",
    "\n",
    "                    hcurves[i_vs30,i_site,i_imt,:,i_stat] = hcurve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['hcurves'] = {}\n",
    "data['hcurves']['hcurves_stats'] = hcurves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = 9.80665 # gravity in m/s^2\n",
    "\n",
    "def acc_to_disp(acc,t):\n",
    "    return (acc * g) * (t/(2*np.pi))**2\n",
    "\n",
    "def period_from_imt(imt):\n",
    "    if imt in ['PGA','PGD']:\n",
    "        period = 0\n",
    "    else:\n",
    "        period = float(re.split('\\(|\\)',imt)[1])\n",
    "    return period\n",
    "\n",
    "def period_from_imt(imt):\n",
    "    if imt in ['PGA','PGD']:\n",
    "        period = 0\n",
    "    else:\n",
    "        period = float(re.split('\\(|\\)',imt)[1])\n",
    "    return period\n",
    "\n",
    "def convert_imtls_to_disp(acc_imtls):\n",
    "    '''\n",
    "    converts the intensity measure types and levels to spectral displacements\n",
    "    '''\n",
    "    disp_imtls = {}\n",
    "    for acc_imt in acc_imtls.keys():\n",
    "        period = period_from_imt(acc_imt)\n",
    "        disp_imt = acc_imt.replace('A','D')\n",
    "\n",
    "        disp_imtls[disp_imt] = acc_to_disp(np.array(acc_imtls[acc_imt]),period).tolist()\n",
    "        \n",
    "    return disp_imtls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_hazard_design_intensities(data,hazard_rps,intensity_type='acc'):\n",
    "    '''\n",
    "    calculate design intensities based on an annual probability of exceedance (APoE)\n",
    "    :param data: dictionary containing hazard curves and metadata for sites, intensity measures, and rlz weights\n",
    "    :param hazard_rps: np array containing the desired return periods (1 / APoE)\n",
    "    :return: np arrays for all intensities from the hazard curve and stats (mean and quantiles)\n",
    "    '''\n",
    "    vs30s = data['metadata']['vs30s']\n",
    "    imtls = data['metadata'][f'{intensity_type}_imtls']    \n",
    "    hcurves_stats   = np.array(data['hcurves']['hcurves_stats'])\n",
    "    \n",
    "    [n_vs30,n_sites,n_imts,n_imtls,n_stats] = hcurves_stats.shape\n",
    "    \n",
    "    n_rps = len(hazard_rps)\n",
    "    \n",
    "    stats_im_hazard = np.zeros([n_vs30,n_sites,n_imts,n_rps,n_stats])\n",
    "    \n",
    "    for i_vs30 in range(n_vs30):\n",
    "        for i_site in range(n_sites):\n",
    "            for i_imt,imt in enumerate(imtls.keys()):\n",
    "\n",
    "                # loop over the median and any quantiles\n",
    "                for i_stat in range(n_stats):\n",
    "                    stats_im_hazard[i_vs30,i_site,i_imt,:,i_stat] = np.exp(np.interp(np.log(1/hazard_rps), np.log(np.flip(hcurves_stats[i_vs30,i_site,i_imt,:,i_stat])), np.log(np.flip(imtls[imt]))))\n",
    "                \n",
    "    return stats_im_hazard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_risk_design_intensities(data,risk_assumptions,intensity_type='acc'):\n",
    "    '''\n",
    "    calculate design intensities based on a risk target and fragility assumptions\n",
    "    :param data: dictionary containing hazard curves and metadata for sites, intensity measures, and rlz weights\n",
    "    :param risk_target_assumptions: dictionary with keys for combinations of assumptions\n",
    "    :param imtl_list: a list of intensity measures to include (must be included in the available imtls)\n",
    "    :return: np arrays for all intensities from the mean hazard curve \n",
    "    '''\n",
    "\n",
    "    intensity_type = 'acc'\n",
    "    vs30s = data['metadata']['vs30s']\n",
    "    imtls = data['metadata'][f'{intensity_type}_imtls']\n",
    "    n_imtls = len(imtls[list(imtls.keys())[0]])\n",
    "    hcurves_stats = np.array(data['hcurves']['hcurves_stats'])\n",
    "\n",
    "    [n_vs30, n_sites, n_imts, n_imtls, n_stats] = hcurves_stats.shape\n",
    "    \n",
    "    n_risk_assumptions = len(risk_assumptions.keys())\n",
    "    \n",
    "    im_risk = np.zeros([n_vs30,n_sites,n_imts,n_risk_assumptions])\n",
    "    lambda_risk = np.zeros_like(im_risk)\n",
    "    fragility_risk = np.zeros_like(im_risk)\n",
    "    disagg_risk = np.zeros([n_vs30,n_sites,n_imts,n_risk_assumptions,n_imtls])\n",
    "    \n",
    "    # select the statistic for the mean\n",
    "    i_stat = 0\n",
    "    \n",
    "    for i_vs30,vs30 in enumerate(vs30s):\n",
    "        print(f'Processing Vs30: {vs30}.')\n",
    "        for i_imt,imt in enumerate(imtls.keys()):\n",
    "            print(f'\\tProcessing {imt}.')\n",
    "            \n",
    "            if imt != 'PGA':\n",
    "                for i_site in range(n_sites):\n",
    "                    # loop over the risk target assumption dictionaries\n",
    "                    for i_rt,rt in enumerate(risk_assumptions.keys()):\n",
    "                        collapse_risk_target = risk_assumptions[rt]['collapse_risk_target']\n",
    "                        cmr = risk_assumptions[rt]['cmr']\n",
    "                        beta = risk_assumptions[rt]['beta']\n",
    "                        design_point = risk_assumptions[rt]['design_point']\n",
    "\n",
    "                        [im_r,median] = find_uniform_risk_intensity(hcurves_stats[i_vs30,i_site,i_imt,:,i_stat], imtls[imt], beta, collapse_risk_target, design_point)\n",
    "                        im_risk[i_vs30,i_site,i_imt,i_rt] = im_r\n",
    "                        lambda_risk[i_vs30,i_site,i_imt,i_rt] = np.interp(im_r, imtls[imt], hcurves_stats[i_vs30,i_site,i_imt,:,i_stat])\n",
    "                        fragility_risk[i_vs30,i_site,i_imt,i_rt] = median\n",
    "\n",
    "                        risk,disagg = risk_convolution(hcurves_stats[i_vs30,i_site,i_imt,:,i_stat], imtls[imt], median, beta)\n",
    "                        disagg_risk[i_vs30,i_site,i_imt,i_rt,:] = disagg\n",
    "                        \n",
    "            else:\n",
    "                for i_site in range(n_sites):\n",
    "                    # loop over the risk target assumption dictionaries\n",
    "                    for i_rt,rt in enumerate(risk_assumptions.keys()):\n",
    "                        hazard_rp = risk_assumptions[rt]['R_rp']\n",
    "                        im_risk[i_vs30,i_site,i_imt,i_rt] = np.exp(np.interp(np.log(1/hazard_rp), np.log(np.flip(hcurves_stats[i_vs30,i_site,i_imt,:,i_stat])), np.log(np.flip(imtls[imt]))))\n",
    "                        lambda_risk[i_vs30,i_site,i_imt,i_rt] =  1 / hazard_rp\n",
    "                        fragility_risk[i_vs30,i_site,i_imt,i_rt] = np.nan\n",
    "                        disagg_risk[i_vs30,i_site,i_imt,i_rt,:] = np.nan\n",
    "        print()\n",
    "\n",
    "    # store results as a dictionary\n",
    "    im_risk = {'im_risk':im_risk,'lambda_risk':lambda_risk,'fragility_risk':fragility_risk,'disagg_risk':disagg_risk}\n",
    "    return im_risk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating PoE values.\n"
     ]
    }
   ],
   "source": [
    "data['metadata']['disp_imtls'] = convert_imtls_to_disp(imtls)\n",
    "data['metadata']['quantiles'] = [float(q) for q in aggs[1:]]\n",
    "\n",
    "# get poe values\n",
    "print('Calculating PoE values.')\n",
    "hazard_rps = np.array([25,50,100,250,500,1000,2500])\n",
    "data['hazard_design'] = {}\n",
    "data['hazard_design']['hazard_rps'] = hazard_rps.tolist()\n",
    "\n",
    "for intensity_type in ['acc','disp']:\n",
    "    data['hazard_design'][intensity_type] = {}\n",
    "    data['hazard_design'][intensity_type]['stats_im_hazard'] = calculate_hazard_design_intensities(data,hazard_rps,intensity_type)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_hdf(hf_name,data):\n",
    "    \n",
    "    with h5py.File(hf_name,'w') as hf:\n",
    "        \n",
    "        # add metadata\n",
    "        grp = hf.create_group('metadata')\n",
    "        grp.attrs['vs30s'] = data['metadata']['vs30s']\n",
    "        grp.attrs['quantiles'] = data['metadata']['quantiles']\n",
    "        grp.attrs['acc_imtls'] = str(data['metadata']['acc_imtls'])\n",
    "        grp.attrs['disp_imtls'] = str(data['metadata']['disp_imtls'])\n",
    "        grp.attrs['sites'] = str(data['metadata']['sites'].to_dict())\n",
    "        \n",
    "        # add hazard curves\n",
    "        grp = hf.create_group('hcurves')\n",
    "        for dset_name in ['hcurves_stats']:\n",
    "            dset = grp.create_dataset(dset_name,np.array(data['hcurves'][dset_name]).shape)\n",
    "            dset[:] = np.array(data['hcurves'][dset_name])\n",
    "            \n",
    "        # add poe values\n",
    "        grp = hf.create_group('hazard_design')\n",
    "        grp.attrs['hazard_rps'] = data['hazard_design']['hazard_rps']\n",
    "        for intensity_type in ['acc','disp']:\n",
    "            subgrp = grp.create_group(intensity_type)\n",
    "            for dset_name in ['stats_im_hazard']:\n",
    "                dset = subgrp.create_dataset(dset_name,np.array(data['hazard_design'][intensity_type][dset_name]).shape)\n",
    "                dset[:] = np.array(data['hazard_design'][intensity_type][dset_name])\n",
    "                \n",
    "        # add risk values\n",
    "        if 'risk_design' in data.keys():\n",
    "            grp = hf.create_group('risk_design')\n",
    "            intensity_type = 'acc'\n",
    "            subgrp = grp.create_group(intensity_type)\n",
    "            subgrp.attrs['risk_assumptions'] = str(data['risk_design'][intensity_type]['risk_assumptions'])\n",
    "            for grp_name in ['im_risk']:\n",
    "                rlzgrp = subgrp.create_group(grp_name)\n",
    "                for dset_name in ['im_risk','lambda_risk','fragility_risk','disagg_risk']:\n",
    "                    dset = rlzgrp.create_dataset(dset_name,np.array(data['risk_design'][intensity_type][grp_name][dset_name]).shape)\n",
    "                    dset[:] = np.array(data['risk_design'][intensity_type][grp_name][dset_name])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_hdf_for_risk(hf_name,data):\n",
    "    \n",
    "    with h5py.File(hf_name,'r+') as hf:\n",
    "        \n",
    "        if 'risk_design' in hf.keys():\n",
    "            del hf['risk_design']\n",
    "                \n",
    "        # add risk values\n",
    "        if 'risk_design' in data.keys():\n",
    "            grp = hf.create_group('risk_design')\n",
    "            intensity_type = 'acc'\n",
    "            subgrp = grp.create_group(intensity_type)\n",
    "            subgrp.attrs['risk_assumptions'] = str(data['risk_design'][intensity_type]['risk_assumptions'])\n",
    "            for grp_name in ['im_risk']:\n",
    "                rlzgrp = subgrp.create_group(grp_name)\n",
    "                for dset_name in ['im_risk','lambda_risk','fragility_risk','disagg_risk']:\n",
    "                    dset = rlzgrp.create_dataset(dset_name,np.array(data['risk_design'][intensity_type][grp_name][dset_name]).shape)\n",
    "                    dset[:] = np.array(data['risk_design'][intensity_type][grp_name][dset_name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'baseline_risk': 1.5e-05,\n",
       " 'R_rps': [500, 1000, 2500],\n",
       " 'APoE: 1/500': {'risk_factor': 1.0, 'risk_target': 1.5e-05},\n",
       " 'APoE: 1/1000': {'risk_factor': 0.5, 'risk_target': 7.5e-06},\n",
       " 'APoE: 1/2500': {'risk_factor': 0.2, 'risk_target': 3e-06}}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "{'APoE: 1/500': {'risk_factor': 1.0,\n",
       "  'risk_target': 1.5e-05,\n",
       "  'R_rp': 500,\n",
       "  'collapse_risk_target': 0.00015,\n",
       "  'cmr': 4,\n",
       "  'beta': 0.45,\n",
       "  'design_point': 0.001032732090754596,\n",
       "  'p_fatality_given_collapse': 0.1},\n",
       " 'APoE: 1/1000': {'risk_factor': 0.5,\n",
       "  'risk_target': 7.5e-06,\n",
       "  'R_rp': 1000,\n",
       "  'collapse_risk_target': 7.5e-05,\n",
       "  'cmr': 4,\n",
       "  'beta': 0.45,\n",
       "  'design_point': 0.001032732090754596,\n",
       "  'p_fatality_given_collapse': 0.1},\n",
       " 'APoE: 1/2500': {'risk_factor': 0.2,\n",
       "  'risk_target': 3e-06,\n",
       "  'R_rp': 2500,\n",
       "  'collapse_risk_target': 3e-05,\n",
       "  'cmr': 4,\n",
       "  'beta': 0.45,\n",
       "  'design_point': 0.001032732090754596,\n",
       "  'p_fatality_given_collapse': 0.1}}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "R_assumptions = {}\n",
    "\n",
    "####\n",
    "baseline_rp = 500\n",
    "baseline_risk = 1.5e-5\n",
    "cmr = 4\n",
    "####\n",
    "\n",
    "beta = 0.45\n",
    "design_point = stats.lognorm(beta,scale=cmr).cdf(1)\n",
    "p_fatality_given_collapse = 0.1\n",
    "\n",
    "R_assumptions['baseline_risk'] = baseline_risk\n",
    "R_assumptions['R_rps'] = [500,1000,2500]\n",
    "for R_rp in R_assumptions['R_rps']:\n",
    "    risk_factor = baseline_rp/R_rp\n",
    "    risk_target = round(baseline_risk*risk_factor,12)\n",
    "    R_assumptions[f'APoE: 1/{R_rp}'] = {'risk_factor': risk_factor,'risk_target': risk_target}\n",
    "\n",
    "R_assumptions\n",
    "\n",
    "\n",
    "risk_assumptions = {}\n",
    "for R_rp in R_assumptions['R_rps']:\n",
    "    key = f'APoE: 1/{R_rp}'\n",
    "    risk_assumptions[key] = { 'risk_factor':R_assumptions[f'APoE: 1/{R_rp}']['risk_factor'],\n",
    "                              'risk_target':R_assumptions[f'APoE: 1/{R_rp}']['risk_target'],\n",
    "                              'R_rp':R_rp,\n",
    "                              'collapse_risk_target': round(R_assumptions[f'APoE: 1/{R_rp}']['risk_target'] / p_fatality_given_collapse,12),\n",
    "                              'cmr':cmr,\n",
    "                              'beta':beta,\n",
    "                              'design_point':design_point,\n",
    "                              'p_fatality_given_collapse':p_fatality_given_collapse\n",
    "                               }\n",
    "    \n",
    "risk_assumptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if data_type=='grid':\n",
    "    hf_name = f'v3_cmr-{cmr}_vs30-{vs30_list}_imt-{imts}{site_label}.hdf5'\n",
    "else:\n",
    "    hf_name = f'v3_cmr-{cmr}_vs30-all_R-factors.hdf5'\n",
    "save_hdf(hf_name,data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating risk informed values.\n",
      "Processing Vs30: 250.\n",
      "\tProcessing SA(0.5).\n",
      "\tProcessing SA(1.5).\n",
      "\n",
      "Processing Vs30: 400.\n",
      "\tProcessing SA(0.5).\n",
      "\tProcessing SA(1.5).\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# get risk values\n",
    "print('Calculating risk informed values.')\n",
    "data['risk_design'] = {}\n",
    "intensity_type = 'acc'\n",
    "data['risk_design'][intensity_type] = {}\n",
    "data['risk_design'][intensity_type]['risk_assumptions'] = risk_assumptions\n",
    "data['risk_design'][intensity_type]['im_risk'] = calculate_risk_design_intensities(data,risk_assumptions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "update_hdf_for_risk(hf_name,data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310",
   "language": "python",
   "name": "py310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
